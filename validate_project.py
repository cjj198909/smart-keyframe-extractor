#!/usr/bin/env python3
"""
é¡¹ç›®å®Œæˆåº¦éªŒè¯å·¥å…·
éªŒè¯Smart Keyframe Extractoré¡¹ç›®çš„æ‰€æœ‰ç»„ä»¶å’ŒåŠŸèƒ½
"""

import os
import json
import subprocess
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
import importlib.util


class ProjectValidator:
    """é¡¹ç›®å®Œæˆåº¦éªŒè¯å™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent
        self.validation_results = {
            'timestamp': datetime.now().isoformat(),
            'project_root': str(self.project_root),
            'validations': {}
        }
    
    def validate_core_modules(self) -> Dict[str, Any]:
        """éªŒè¯æ ¸å¿ƒæ¨¡å—"""
        print("ğŸ” éªŒè¯æ ¸å¿ƒæ¨¡å—...")
        
        core_modules = {
            'extractor.py': 'ä¸»è¦æå–å™¨æ¨¡å—',
            'azure_openai.py': 'Azure OpenAIé›†æˆ',
            'vision_utils.py': 'è§†è§‰å¤„ç†å·¥å…·',
            'cli.py': 'å‘½ä»¤è¡Œæ¥å£'
        }
        
        results = {'status': 'success', 'modules': {}, 'issues': []}
        
        for module_file, description in core_modules.items():
            module_path = self.project_root / 'smart_keyframe_extractor' / module_file
            
            if module_path.exists():
                try:
                    # å°è¯•å¯¼å…¥æ¨¡å—éªŒè¯è¯­æ³•
                    spec = importlib.util.spec_from_file_location(
                        module_file.replace('.py', ''), module_path
                    )
                    module = importlib.util.module_from_spec(spec)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆåº”è¯¥æœ‰å®é™…å†…å®¹ï¼‰
                    file_size = module_path.stat().st_size
                    
                    results['modules'][module_file] = {
                        'status': 'âœ… å­˜åœ¨ä¸”æœ‰æ•ˆ',
                        'description': description,
                        'file_size': file_size,
                        'path': str(module_path)
                    }
                    
                except Exception as e:
                    results['modules'][module_file] = {
                        'status': 'âŒ è¯­æ³•é”™è¯¯',
                        'error': str(e),
                        'description': description
                    }
                    results['issues'].append(f"{module_file}: {e}")
            else:
                results['modules'][module_file] = {
                    'status': 'âŒ ç¼ºå¤±',
                    'description': description
                }
                results['issues'].append(f"{module_file}: æ–‡ä»¶ä¸å­˜åœ¨")
        
        if results['issues']:
            results['status'] = 'warning'
        
        return results
    
    def validate_test_suite(self) -> Dict[str, Any]:
        """éªŒè¯æµ‹è¯•å¥—ä»¶"""
        print("ğŸ§ª éªŒè¯æµ‹è¯•å¥—ä»¶...")
        
        test_tools = {
            'quick_benchmark.py': 'å¿«é€ŸåŸºå‡†æµ‹è¯•',
            'concurrent_stress_test.py': 'å¹¶å‘å‹åŠ›æµ‹è¯•',
            'cloud_stress_test.py': 'äº‘æœåŠ¡å™¨æµ‹è¯•',
            'memory_stress_test.py': 'å†…å­˜å‹åŠ›æµ‹è¯•',
            'stress_test.py': 'å®Œæ•´å‹åŠ›æµ‹è¯•',
            'performance_optimizer.py': 'æ€§èƒ½ä¼˜åŒ–å™¨',
            'branch_comparator.py': 'åˆ†æ”¯å¯¹æ¯”å·¥å…·',
            'intelligent_batch_processor.py': 'æ™ºèƒ½æ‰¹å¤„ç†å™¨'
        }
        
        results = {'status': 'success', 'tools': {}, 'issues': []}
        
        for tool_file, description in test_tools.items():
            tool_path = self.project_root / 'benchmark' / tool_file
            
            if tool_path.exists():
                file_size = tool_path.stat().st_size
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¯æ‰§è¡Œè„šæœ¬
                try:
                    with open(tool_path, 'r') as f:
                        content = f.read()
                        has_main = 'def main(' in content or 'if __name__ == "__main__"' in content
                        has_shebang = content.startswith('#!')
                        
                    results['tools'][tool_file] = {
                        'status': 'âœ… å®Œæ•´',
                        'description': description,
                        'file_size': file_size,
                        'executable': has_main and has_shebang,
                        'path': str(tool_path)
                    }
                    
                except Exception as e:
                    results['tools'][tool_file] = {
                        'status': 'âš ï¸ å¯èƒ½æœ‰é—®é¢˜',
                        'error': str(e),
                        'description': description
                    }
                    results['issues'].append(f"{tool_file}: {e}")
            else:
                results['tools'][tool_file] = {
                    'status': 'âŒ ç¼ºå¤±',
                    'description': description
                }
                results['issues'].append(f"{tool_file}: æ–‡ä»¶ä¸å­˜åœ¨")
        
        if results['issues']:
            results['status'] = 'warning'
        
        return results
    
    def validate_documentation(self) -> Dict[str, Any]:
        """éªŒè¯æ–‡æ¡£å®Œæ•´æ€§"""
        print("ğŸ“š éªŒè¯æ–‡æ¡£...")
        
        required_docs = {
            'README.md': 'é¡¹ç›®ä¸»è¦è¯´æ˜æ–‡æ¡£',
            'STRESS_TESTING_GUIDE.md': 'å‹åŠ›æµ‹è¯•æŒ‡å—',
            'PERFORMANCE_ANALYSIS_REPORT.md': 'æ€§èƒ½åˆ†ææŠ¥å‘Š',
            'STRESS_TEST_COMPLETION_SUMMARY.md': 'æµ‹è¯•å®Œæˆæ€»ç»“',
            'benchmark/README.md': 'æµ‹è¯•å·¥å…·è¯´æ˜',
            'benchmark/CONCURRENT_TESTING_GUIDE.md': 'å¹¶å‘æµ‹è¯•æŒ‡å—'
        }
        
        results = {'status': 'success', 'documents': {}, 'issues': []}
        
        for doc_file, description in required_docs.items():
            doc_path = self.project_root / doc_file
            
            if doc_path.exists():
                file_size = doc_path.stat().st_size
                
                # æ£€æŸ¥æ–‡æ¡£å†…å®¹è´¨é‡
                try:
                    with open(doc_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        line_count = len(content.splitlines())
                        word_count = len(content.split())
                        
                    quality = 'excellent' if word_count > 1000 else 'good' if word_count > 500 else 'basic'
                    
                    results['documents'][doc_file] = {
                        'status': 'âœ… å®Œæ•´',
                        'description': description,
                        'file_size': file_size,
                        'lines': line_count,
                        'words': word_count,
                        'quality': quality
                    }
                    
                except Exception as e:
                    results['documents'][doc_file] = {
                        'status': 'âš ï¸ è¯»å–é”™è¯¯',
                        'error': str(e),
                        'description': description
                    }
                    results['issues'].append(f"{doc_file}: {e}")
            else:
                results['documents'][doc_file] = {
                    'status': 'âŒ ç¼ºå¤±',
                    'description': description
                }
                results['issues'].append(f"{doc_file}: æ–‡ä»¶ä¸å­˜åœ¨")
        
        if results['issues']:
            results['status'] = 'warning'
        
        return results
    
    def validate_test_results(self) -> Dict[str, Any]:
        """éªŒè¯æµ‹è¯•ç»“æœ"""
        print("ğŸ“Š éªŒè¯æµ‹è¯•ç»“æœ...")
        
        results_dirs = [
            ('wyze_auto_results', 'å¤§è§„æ¨¡å‹åŠ›æµ‹è¯•ç»“æœ'),
            ('benchmark_results', 'åŸºå‡†æµ‹è¯•ç»“æœ'),
            ('cloud_stress_results', 'äº‘æœåŠ¡å™¨æµ‹è¯•ç»“æœ')
        ]
        
        results = {'status': 'success', 'result_directories': {}, 'issues': []}
        
        for dir_name, description in results_dirs:
            dir_path = self.project_root / dir_name
            
            if dir_path.exists():
                # ç»Ÿè®¡ç»“æœæ–‡ä»¶
                json_files = list(dir_path.glob('*.json'))
                csv_files = list(dir_path.glob('*.csv'))
                log_files = list(dir_path.glob('*.log'))
                
                # æŸ¥æ‰¾æœ€æ–°çš„æ±‡æ€»æŠ¥å‘Š
                summary_files = list(dir_path.glob('summary_report_*.json'))
                latest_summary = None
                
                if summary_files:
                    latest_summary = max(summary_files, key=os.path.getctime)
                    
                    try:
                        with open(latest_summary, 'r') as f:
                            summary_data = json.load(f)
                            
                        total_tasks = summary_data.get('summary', {}).get('total_tasks', 0)
                        success_rate = summary_data.get('summary', {}).get('success_rate', 0)
                        
                        results['result_directories'][dir_name] = {
                            'status': 'âœ… æœ‰æœ‰æ•ˆç»“æœ',
                            'description': description,
                            'json_files': len(json_files),
                            'csv_files': len(csv_files),
                            'log_files': len(log_files),
                            'latest_summary': str(latest_summary.name),
                            'total_tasks': total_tasks,
                            'success_rate': f"{success_rate:.1f}%"
                        }
                        
                    except Exception as e:
                        results['result_directories'][dir_name] = {
                            'status': 'âš ï¸ ç»“æœå¯èƒ½æŸå',
                            'error': str(e),
                            'description': description
                        }
                        results['issues'].append(f"{dir_name}: æ±‡æ€»æŠ¥å‘Šè¯»å–é”™è¯¯: {e}")
                else:
                    results['result_directories'][dir_name] = {
                        'status': 'ğŸŸ¡ ç›®å½•å­˜åœ¨ä½†æ— æ±‡æ€»æŠ¥å‘Š',
                        'description': description,
                        'json_files': len(json_files),
                        'csv_files': len(csv_files),
                        'log_files': len(log_files)
                    }
                    results['issues'].append(f"{dir_name}: ç¼ºå°‘æ±‡æ€»æŠ¥å‘Š")
            else:
                results['result_directories'][dir_name] = {
                    'status': 'âŒ ç›®å½•ä¸å­˜åœ¨',
                    'description': description
                }
                results['issues'].append(f"{dir_name}: ç›®å½•ä¸å­˜åœ¨")
        
        if results['issues']:
            results['status'] = 'warning'
        
        return results
    
    def validate_deployment_readiness(self) -> Dict[str, Any]:
        """éªŒè¯éƒ¨ç½²å°±ç»ªçŠ¶æ€"""
        print("ğŸš€ éªŒè¯éƒ¨ç½²å°±ç»ªçŠ¶æ€...")
        
        deployment_files = {
            'pyproject.toml': 'é¡¹ç›®é…ç½®æ–‡ä»¶',
            'setup.py': 'Pythonå®‰è£…è„šæœ¬ï¼ˆå¯é€‰ï¼‰',
            'requirements.txt': 'ä¾èµ–æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰',
            'MANIFEST.in': 'æ‰“åŒ…æ¸…å•',
            'LICENSE': 'è®¸å¯è¯æ–‡ä»¶',
            'deploy_cloud_stress_test.sh': 'äº‘éƒ¨ç½²è„šæœ¬',
            'upload_to_github.sh': 'GitHubä¸Šä¼ è„šæœ¬'
        }
        
        results = {'status': 'success', 'deployment_files': {}, 'issues': []}
        
        for file_name, description in deployment_files.items():
            file_path = self.project_root / file_name
            
            if file_path.exists():
                file_size = file_path.stat().st_size
                
                results['deployment_files'][file_name] = {
                    'status': 'âœ… å­˜åœ¨',
                    'description': description,
                    'file_size': file_size
                }
            else:
                if file_name in ['setup.py', 'requirements.txt']:
                    results['deployment_files'][file_name] = {
                        'status': 'ğŸŸ¡ å¯é€‰æ–‡ä»¶ç¼ºå¤±',
                        'description': description
                    }
                else:
                    results['deployment_files'][file_name] = {
                        'status': 'âŒ ç¼ºå¤±',
                        'description': description
                    }
                    results['issues'].append(f"{file_name}: éƒ¨ç½²æ–‡ä»¶ç¼ºå¤±")
        
        # æ£€æŸ¥GitçŠ¶æ€
        try:
            git_status = subprocess.run(['git', 'status', '--porcelain'], 
                                      capture_output=True, text=True, 
                                      cwd=self.project_root)
            
            if git_status.returncode == 0:
                uncommitted_files = git_status.stdout.strip().splitlines()
                results['git_status'] = {
                    'repository_exists': True,
                    'uncommitted_files': len(uncommitted_files),
                    'clean_working_directory': len(uncommitted_files) == 0
                }
            else:
                results['git_status'] = {
                    'repository_exists': False,
                    'error': 'Not a git repository'
                }
                
        except Exception as e:
            results['git_status'] = {
                'repository_exists': False,
                'error': str(e)
            }
        
        if results['issues']:
            results['status'] = 'warning'
        
        return results
    
    def run_full_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("ğŸ” Smart Keyframe Extractor - é¡¹ç›®å®Œæˆåº¦éªŒè¯")
        print("=" * 70)
        
        # æ‰§è¡Œå„é¡¹éªŒè¯
        validations = {
            'core_modules': self.validate_core_modules(),
            'test_suite': self.validate_test_suite(),
            'documentation': self.validate_documentation(),
            'test_results': self.validate_test_results(),
            'deployment': self.validate_deployment_readiness()
        }
        
        self.validation_results['validations'] = validations
        
        # è®¡ç®—æ€»ä½“çŠ¶æ€
        all_statuses = [v['status'] for v in validations.values()]
        if all(status == 'success' for status in all_statuses):
            overall_status = 'excellent'
        elif any(status == 'warning' for status in all_statuses):
            overall_status = 'good'
        else:
            overall_status = 'needs_improvement'
        
        self.validation_results['overall_status'] = overall_status
        
        return self.validation_results
    
    def print_validation_report(self, validation_results: Dict = None):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        if validation_results is None:
            validation_results = self.validation_results
        
        print(f"\nğŸ“‹ é¡¹ç›®éªŒè¯æŠ¥å‘Š")
        print("=" * 70)
        
        validations = validation_results['validations']
        
        # æ ¸å¿ƒæ¨¡å—éªŒè¯
        print(f"\nğŸ”§ æ ¸å¿ƒæ¨¡å—éªŒè¯:")
        core_results = validations['core_modules']
        for module, info in core_results['modules'].items():
            print(f"  {info['status']} {module}: {info['description']}")
        
        # æµ‹è¯•å¥—ä»¶éªŒè¯
        print(f"\nğŸ§ª æµ‹è¯•å¥—ä»¶éªŒè¯:")
        test_results = validations['test_suite']
        for tool, info in test_results['tools'].items():
            print(f"  {info['status']} {tool}: {info['description']}")
        
        # æ–‡æ¡£éªŒè¯
        print(f"\nğŸ“š æ–‡æ¡£éªŒè¯:")
        doc_results = validations['documentation']
        for doc, info in doc_results['documents'].items():
            status_detail = ""
            if 'words' in info:
                status_detail = f" ({info['words']} è¯, {info['quality']})"
            print(f"  {info['status']} {doc}: {info['description']}{status_detail}")
        
        # æµ‹è¯•ç»“æœéªŒè¯
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœéªŒè¯:")
        result_validation = validations['test_results']
        for dir_name, info in result_validation['result_directories'].items():
            extra_info = ""
            if 'total_tasks' in info:
                extra_info = f" ({info['total_tasks']} ä»»åŠ¡, {info['success_rate']} æˆåŠŸç‡)"
            print(f"  {info['status']} {dir_name}: {info['description']}{extra_info}")
        
        # éƒ¨ç½²å°±ç»ªéªŒè¯
        print(f"\nğŸš€ éƒ¨ç½²å°±ç»ªéªŒè¯:")
        deploy_results = validations['deployment']
        for file_name, info in deploy_results['deployment_files'].items():
            print(f"  {info['status']} {file_name}: {info['description']}")
        
        if 'git_status' in deploy_results:
            git_info = deploy_results['git_status']
            if git_info['repository_exists']:
                git_status = "âœ… Gitä»“åº“å°±ç»ª" if git_info['clean_working_directory'] else f"ğŸŸ¡ æœ‰ {git_info['uncommitted_files']} ä¸ªæœªæäº¤æ–‡ä»¶"
                print(f"  {git_status}")
        
        # æ€»ä½“è¯„ä¼°
        overall = validation_results['overall_status']
        print(f"\nğŸ¯ æ€»ä½“è¯„ä¼°:")
        if overall == 'excellent':
            print("  ğŸ‰ ä¼˜ç§€ï¼é¡¹ç›®å®Œæˆåº¦æé«˜ï¼Œå·²è¾¾åˆ°ç”Ÿäº§ç¯å¢ƒæ ‡å‡†")
            print("  âœ… æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å®Œæ•´")
            print("  âœ… æµ‹è¯•è¦†ç›–ç‡å……åˆ†")  
            print("  âœ… æ–‡æ¡£å®Œå–„")
            print("  âœ… éƒ¨ç½²å°±ç»ª")
        elif overall == 'good':
            print("  ğŸ‘ è‰¯å¥½ï¼é¡¹ç›®åŸºæœ¬å®Œæˆï¼Œæœ‰å°‘é‡å¾…æ”¹è¿›é¡¹")
            print("  ğŸ”§ å»ºè®®è§£å†³è­¦å‘Šé¡¹åå‘å¸ƒ")
        else:
            print("  âš ï¸ éœ€è¦æ”¹è¿›ï¼å­˜åœ¨å…³é”®é—®é¢˜éœ€è¦è§£å†³")
        
        # æ”¶é›†æ‰€æœ‰é—®é¢˜
        all_issues = []
        for validation in validations.values():
            all_issues.extend(validation.get('issues', []))
        
        if all_issues:
            print(f"\nâ— å‘ç°çš„é—®é¢˜:")
            for issue in all_issues:
                print(f"  â€¢ {issue}")
        
        print(f"\nğŸ“… éªŒè¯æ—¶é—´: {validation_results['timestamp'][:19]}")
    
    def save_validation_report(self, output_file: str = None):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"project_validation_report_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é¡¹ç›®å®Œæˆåº¦éªŒè¯å·¥å…·')
    parser.add_argument('--project-root', help='é¡¹ç›®æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--save-report', help='ä¿å­˜éªŒè¯æŠ¥å‘Šåˆ°æŒ‡å®šæ–‡ä»¶')
    parser.add_argument('--quiet', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œä»…è¾“å‡ºç»“æœ')
    
    args = parser.parse_args()
    
    if not args.quiet:
        print("ğŸ” å¯åŠ¨é¡¹ç›®å®Œæˆåº¦éªŒè¯...")
    
    validator = ProjectValidator(args.project_root)
    validation_results = validator.run_full_validation()
    
    if not args.quiet:
        validator.print_validation_report(validation_results)
    
    if args.save_report:
        validator.save_validation_report(args.save_report)
    
    # è¿”å›éªŒè¯çŠ¶æ€ä½œä¸ºé€€å‡ºç 
    overall_status = validation_results['overall_status']
    if overall_status == 'excellent':
        sys.exit(0)  # ä¼˜ç§€
    elif overall_status == 'good':
        sys.exit(1)  # è‰¯å¥½ä½†æœ‰è­¦å‘Š
    else:
        sys.exit(2)  # éœ€è¦æ”¹è¿›


if __name__ == "__main__":
    main()
