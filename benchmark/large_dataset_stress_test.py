#!/usr/bin/env python3
"""
å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†å‹åŠ›æµ‹è¯•
Large Dataset Stress Testing

ä¸“é—¨é’ˆå¯¹å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†çš„å‹åŠ›æµ‹è¯•å·¥å…·
"""

import os
import sys
import argparse
import time
import json
from pathlib import Path
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from smart_keyframe_extractor.extractor import extract_top_k_keyframes

class LargeDatasetStressTester:
    """å¤§è§„æ¨¡æ•°æ®é›†å‹åŠ›æµ‹è¯•å™¨"""
    
    def __init__(self, video_dirs: List[str], output_dir: str = "test_auto_results"):
        self.video_dirs = video_dirs
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
    def discover_videos(self) -> List[Path]:
        """å‘ç°æ‰€æœ‰è§†é¢‘æ–‡ä»¶"""
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv'}
        video_files = []
        
        for video_dir in self.video_dirs:
            video_path = Path(video_dir)
            if not video_path.exists():
                self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {video_dir}")
                continue
                
            for file_path in video_path.rglob('*'):
                if file_path.suffix.lower() in video_extensions:
                    video_files.append(file_path)
        
        self.logger.info(f"å‘ç° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        return sorted(video_files)
    
    def process_single_video(self, video_path: Path, config: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        start_time = time.time()
        
        try:
            # æå–å…³é”®å¸§
            result = extract_top_k_keyframes(
                str(video_path),
                k=config.get('k', 3),
                save_files=False,  # ä¸ä¿å­˜æ–‡ä»¶ï¼Œä»…æµ‹è¯•æ€§èƒ½
                resolution=config.get('resolution', '720p')
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            return {
                'video_path': str(video_path),
                'video_name': video_path.name,
                'status': 'success',
                'processing_time': processing_time,
                'keyframes_count': len(result['keyframes']),
                'config': config,
                'file_size_mb': video_path.stat().st_size / (1024 * 1024),
                'error': None
            }
            
        except Exception as e:
            end_time = time.time()
            processing_time = end_time - start_time
            
            return {
                'video_path': str(video_path),
                'video_name': video_path.name,
                'status': 'failed',
                'processing_time': processing_time,
                'keyframes_count': 0,
                'config': config,
                'file_size_mb': video_path.stat().st_size / (1024 * 1024) if video_path.exists() else 0,
                'error': str(e)
            }
    
    def run_concurrent_test(self, video_files: List[Path], max_workers: int = 4) -> List[Dict[str, Any]]:
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        test_config = {
            'k': 3,
            'resolution': '720p'
        }
        
        results = []
        
        self.logger.info(f"å¼€å§‹å¹¶å‘æµ‹è¯•ï¼Œæœ€å¤§workeræ•°: {max_workers}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_video = {
                executor.submit(self.process_single_video, video_file, test_config): video_file 
                for video_file in video_files
            }
            
            # æ”¶é›†ç»“æœ
            for i, future in enumerate(as_completed(future_to_video), 1):
                video_file = future_to_video[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    status_icon = "âœ…" if result['status'] == 'success' else "âŒ"
                    self.logger.info(
                        f"{status_icon} [{i}/{len(video_files)}] {video_file.name} - "
                        f"{result['processing_time']:.2f}s"
                    )
                    
                except Exception as e:
                    self.logger.error(f"å¤„ç† {video_file.name} æ—¶å‡ºé”™: {e}")
                    results.append({
                        'video_path': str(video_file),
                        'video_name': video_file.name,
                        'status': 'error',
                        'processing_time': 0,
                        'keyframes_count': 0,
                        'config': test_config,
                        'file_size_mb': 0,
                        'error': str(e)
                    })
        
        return results
    
    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        total_videos = len(results)
        successful_videos = sum(1 for r in results if r['status'] == 'success')
        failed_videos = total_videos - successful_videos
        
        if successful_videos > 0:
            successful_results = [r for r in results if r['status'] == 'success']
            total_processing_time = sum(r['processing_time'] for r in successful_results)
            avg_processing_time = total_processing_time / successful_videos
            total_file_size = sum(r['file_size_mb'] for r in successful_results)
            avg_file_size = total_file_size / successful_videos
            processing_speed = total_file_size / total_processing_time if total_processing_time > 0 else 0
        else:
            avg_processing_time = 0
            avg_file_size = 0
            processing_speed = 0
            total_processing_time = 0
        
        analysis = {
            'test_summary': {
                'total_videos': total_videos,
                'successful_videos': successful_videos,
                'failed_videos': failed_videos,
                'success_rate': (successful_videos / total_videos * 100) if total_videos > 0 else 0
            },
            'performance_metrics': {
                'total_processing_time': total_processing_time,
                'avg_processing_time': avg_processing_time,
                'avg_file_size_mb': avg_file_size,
                'processing_speed_mb_per_sec': processing_speed
            },
            'detailed_results': results
        }
        
        return analysis
    
    def save_results(self, analysis: Dict[str, Any], test_name: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if test_name is None:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            test_name = f"large_dataset_test_{timestamp}"
        
        # ä¿å­˜JSONæ ¼å¼
        json_file = self.output_dir / f"{test_name}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ ¼å¼
        csv_file = self.output_dir / f"{test_name}.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write("video_name,status,processing_time,keyframes_count,file_size_mb,error\n")
            for result in analysis['detailed_results']:
                f.write(f"{result['video_name']},{result['status']},{result['processing_time']:.2f},"
                       f"{result['keyframes_count']},{result['file_size_mb']:.2f},\"{result['error'] or ''}\"\n")
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {json_file} å’Œ {csv_file}")
    
    def print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        summary = analysis['test_summary']
        metrics = analysis['performance_metrics']
        
        print("\n" + "="*60)
        print("ğŸ¬ å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†å‹åŠ›æµ‹è¯•ç»“æœ")
        print("="*60)
        
        print(f"\nğŸ“Š æµ‹è¯•æ¦‚è§ˆ:")
        print(f"  æ€»è§†é¢‘æ•°: {summary['total_videos']}")
        print(f"  æˆåŠŸå¤„ç†: {summary['successful_videos']}")
        print(f"  å¤±è´¥æ•°é‡: {summary['failed_videos']}")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        
        if summary['successful_videos'] > 0:
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            print(f"  æ€»å¤„ç†æ—¶é—´: {metrics['total_processing_time']:.1f} ç§’")
            print(f"  å¹³å‡å¤„ç†æ—¶é—´: {metrics['avg_processing_time']:.2f} ç§’/è§†é¢‘")
            print(f"  å¹³å‡æ–‡ä»¶å¤§å°: {metrics['avg_file_size_mb']:.2f} MB")
            print(f"  å¤„ç†é€Ÿåº¦: {metrics['processing_speed_mb_per_sec']:.2f} MB/ç§’")
        
        # æ˜¾ç¤ºå¤±è´¥çš„è§†é¢‘
        failed_results = [r for r in analysis['detailed_results'] if r['status'] != 'success']
        if failed_results:
            print(f"\nâŒ å¤±è´¥çš„è§†é¢‘:")
            for result in failed_results[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - {result['video_name']}: {result['error']}")
            if len(failed_results) > 5:
                print(f"  ... è¿˜æœ‰ {len(failed_results) - 5} ä¸ªå¤±è´¥çš„è§†é¢‘")

def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    test_dirs = [
        "videos",  # é¡¹ç›®å†…çš„æµ‹è¯•è§†é¢‘
        "/tmp/test_videos"  # å¯é€‰çš„å¤–éƒ¨æµ‹è¯•ç›®å½•
    ]
    
    # è¿‡æ»¤å­˜åœ¨çš„ç›®å½•
    existing_dirs = [d for d in test_dirs if Path(d).exists()]
    
    print("ğŸ¬ å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†å‹åŠ›æµ‹è¯•")
    print("="*50)
    
    if not existing_dirs:
        print("âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•è§†é¢‘ç›®å½•")
        print("è¯·å°†æµ‹è¯•è§†é¢‘æ”¾åœ¨ä»¥ä¸‹ç›®å½•ä¹‹ä¸€:")
        for d in test_dirs:
            print(f"  - {d}")
        return []
    
    return existing_dirs

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†å‹åŠ›æµ‹è¯•')
    parser.add_argument('--video-dirs', nargs='+', 
                       help='è§†é¢‘ç›®å½•è·¯å¾„åˆ—è¡¨')
    parser.add_argument('--max-workers', type=int, default=4,
                       help='æœ€å¤§å¹¶å‘workeræ•° (é»˜è®¤: 4)')
    parser.add_argument('--output-dir', default='test_auto_results',
                       help='ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: test_auto_results)')
    
    args = parser.parse_args()
    
    # ç¡®å®šè§†é¢‘ç›®å½•
    if args.video_dirs:
        video_dirs = args.video_dirs
    else:
        video_dirs = setup_test_environment()
        if not video_dirs:
            return
    
    # è¿è¡Œæµ‹è¯•
    tester = LargeDatasetStressTester(video_dirs, args.output_dir)
    
    # å‘ç°è§†é¢‘æ–‡ä»¶
    video_files = tester.discover_videos()
    if not video_files:
        print("âŒ æœªå‘ç°ä»»ä½•è§†é¢‘æ–‡ä»¶")
        return
    
    # è¿è¡Œæµ‹è¯•
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯• {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶...")
    start_time = time.time()
    
    results = tester.run_concurrent_test(video_files, args.max_workers)
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # åˆ†æç»“æœ
    analysis = tester.analyze_results(results)
    analysis['test_info'] = {
        'total_test_time': total_time,
        'max_workers': args.max_workers,
        'video_directories': video_dirs
    }
    
    # ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
    tester.save_results(analysis)
    tester.print_summary(analysis)
    
    print(f"\nâ±ï¸  æ€»æµ‹è¯•æ—¶é—´: {total_time:.1f} ç§’")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output_dir}/")

if __name__ == "__main__":
    main()
